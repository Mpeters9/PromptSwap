"use server";

import { revalidatePath } from "next/cache";
import { prisma } from "@/lib/prisma";

export type ChatMessageDTO = {
  id: string;
  role: string;
  content: string;
  createdAt: Date;
};

export type ChatSessionDTO = {
  id: string;
  title: string | null;
  createdAt: Date;
};

function buildLocalMockReply(history: ChatMessageDTO[], userText: string): string {
  const lastUser =
    [...history].reverse().find((m) => m.role !== "assistant")?.content ?? userText;

  const snippets: string[] = [];

  snippets.push(
    "I'm a local mock assistant running without a paid OpenAI key. This reply is generated by your own app logic.",
  );

  if (lastUser) {
    snippets.push(`You said: "${lastUser}"`);
  }

  snippets.push(
    "In a production setup, this is where a real OpenAI model (like gpt-4o-mini) would respond.",
  );

  return snippets.join(" ");
}

// ---- Helper: load a session with messages ----

export async function getChatSessionWithMessages(sessionId: string) {
  if (!sessionId) {
    throw new Error("Missing sessionId");
  }

  const session = await prisma.chatSession.findUnique({
    where: { id: sessionId },
  });

  if (!session) {
    return null;
  }

  const messages = await prisma.chatMessage.findMany({
    where: { sessionId },
    orderBy: { createdAt: "asc" },
  });

  return {
    session: {
      id: session.id,
      title: session.title,
      createdAt: session.createdAt,
    } as ChatSessionDTO,
    messages: messages as ChatMessageDTO[],
  };
}

// ---- Helper: talk to OpenAI (non-streaming for now) ----

async function callOpenAIChatCompletion(
  history: ChatMessageDTO[],
  userText: string,
): Promise<string> {
  const apiKey = process.env.OPENAI_API_KEY;
  const useMock = process.env.USE_MOCK_OPENAI === "1";

  // FREE MODE: If mock flag is on OR no API key, never hit the network.
  if (useMock || !apiKey) {
    if (!apiKey) {
      console.warn("[actions] OPENAI_API_KEY missing. Using local mock reply.");
    } else {
      console.log("[actions] USE_MOCK_OPENAI=1 → using local mock reply.");
    }
    return buildLocalMockReply(history, userText);
  }

  // Build a minimal conversation using the last few messages
  const lastMessages = history.slice(-10);
  const messages = [
    {
      role: "system",
      content:
        "You are an AI chatting inside the PromptSwap app. Be concise and helpful.",
    },
    ...lastMessages.map((m) => ({
      role: m.role === "assistant" ? "assistant" : "user",
      content: m.content,
    })),
    { role: "user", content: userText },
  ];

  const resp = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "gpt-4o-mini",
      messages,
      max_tokens: 512,
    }),
  });

  const errorText = !resp.ok ? await resp.text() : null;

  if (!resp.ok) {
    console.error("[actions] OpenAI error", resp.status, errorText);

    // Try to detect "insufficient_quota" or similar
    let parsed: any = null;
    try {
      parsed = errorText && JSON.parse(errorText);
    } catch {
      // ignore JSON parse failure
    }

    const isQuotaError =
      resp.status === 429 &&
      parsed &&
      parsed.error &&
      parsed.error.code === "insufficient_quota";

    const isAuthError = resp.status === 401 || resp.status === 403;

    if (isQuotaError || isAuthError) {
      // HARD FALLBACK: if quota/auth fails, use local mock so the app keeps working for free.
      console.warn(
        "[actions] Falling back to local mock reply due to OpenAI quota/auth error.",
      );
      return buildLocalMockReply(history, userText);
    }

    // In development, surface the actual error for debugging
    if (process.env.NODE_ENV === "development") {
      const preview = (errorText ?? "").slice(0, 300);
      return `OpenAI error (status ${resp.status}): ${preview}`;
    }

    // Generic message for other errors in production
    return "I hit an error talking to OpenAI. Please try again in a moment.";
  }

  const json = await resp.json();
  const content =
    json.choices?.[0]?.message?.content ??
    "I couldn’t generate a response this time.";

  return content;
}

// ---- Server action: user sends a message ----

export async function sendUserMessage(formData: FormData) {
  const sessionId = formData.get("sessionId")?.toString();
  const content = formData.get("content")?.toString()?.trim();

  if (!sessionId) {
    throw new Error("Missing sessionId");
  }
  if (!content) {
    // Nothing to send
    return;
  }

  // 1) Store the user message
  const userMessage = await prisma.chatMessage.create({
    data: {
      sessionId,
      role: "user",
      content,
    },
  });

  // 2) Reload history (including the message we just added)
  const historyMessages = await prisma.chatMessage.findMany({
    where: { sessionId },
    orderBy: { createdAt: "asc" },
  });

  // 3) Call OpenAI to get an assistant reply
  let assistantContent = "";
  try {
    assistantContent = await callOpenAIChatCompletion(
      historyMessages as ChatMessageDTO[],
      content,
    );
  } catch (err) {
    console.error("[actions] callOpenAIChatCompletion failed:", err);
    assistantContent =
      "I ran into an internal error when trying to generate a response.";
  }

  // 4) Store the assistant message
  await prisma.chatMessage.create({
    data: {
      sessionId,
      role: "assistant",
      content: assistantContent,
    },
  });

  // 5) Ask Next.js to revalidate this chat page
  revalidatePath(`/chat/${sessionId}`);

  // Optional: return something if you want the client to inspect it
  return {
    ok: true,
    userMessageId: userMessage.id,
  };
}
